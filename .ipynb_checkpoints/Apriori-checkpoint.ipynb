{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FQ6FjJ2NEoFS"
   },
   "source": [
    "## Apriori Algorithm\n",
    "\n",
    "### This is the part of Data Mining project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKVTACJGFGYw"
   },
   "source": [
    "We're going to examine movies using our understanding of association rules, to find movies that \"go together\". For this part, you will implement the apriori algorithm, and apply it to a movie rating dataset. We'll use the [MovieLens](https://grouplens.org/datasets/movielens/) dataset.\n",
    "\n",
    "First, run the next cell to load the dataset we are going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "fcZSAoCAFDyA"
   },
   "outputs": [],
   "source": [
    "import urllib3\n",
    "import zipfile\n",
    "\n",
    "http = urllib3.PoolManager()\n",
    "req = http.request(\"GET\", \"https://files.grouplens.org/datasets/movielens/ml-latest-small.zip\", preload_content=False)\n",
    "\n",
    "with open(\"movie.zip\", 'wb') as out:\n",
    "  while True:\n",
    "    data = req.read(4096)\n",
    "    if not data:\n",
    "      break\n",
    "    out.write(data)\n",
    "req.release_conn()\n",
    "\n",
    "zFile = zipfile.ZipFile(\"movie.zip\", \"r\")\n",
    "for fileM in zFile.namelist():\n",
    "  zFile.extract(fileM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "YgIIGQ3lIgRH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.txt  links.csv   movies.csv  ratings.csv tags.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls ml-latest-small/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RiF1Gc0q7qzj"
   },
   "source": [
    "In this dataset, there are four columns: `userId` is the integer ids of users, `movieId` is the integer ids of movies, `rating` is the rate of the user gives to the movie, and `timestamp` which we do not use here. Each row denotes that the user of given `userId` rated the movie of the given `movieId`. We are going to treat each user as a \"basket\", so you will need to collect all the movies that have been rated by a single user as a basket. \n",
    "\n",
    "Now, you need to implement the apriori algorithm and apply it to this dataset to find association rules of user rating behaviors where:\n",
    "\n",
    "1. Define `rating` >= 3 is \"like\" (that is, only consider movie ratings of 3 or higher in your baskets; you may ignore all others)\n",
    "2. `minsup` == 40 (out of 600 users/baskets); we may adjust this based on the discussion on Campuswire\n",
    "3. `minconf` == to be determined by a discussion on Campuswire. You may try several different choices, but we will converge on a good choice for everyone for the final submission.\n",
    " \n",
    "We know there are many existing implementations of apriori online (check github for some good starting points). You are welcome to read existing codebases and let that inform your approach. Do not copy-paste any existing code. We want your code to have sufficient comments to explain your steps, to show us that you really know what you are doing. Furthermore, you should add print statements to print out the intermediate steps of your method -- e.g., the size of the candidate set at each step of the method, the size of the filtered set, and any other important information you think will highlight the method. \n",
    "\n",
    "To help get you started, we can load the ratings with the following code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "0y8yZnEVI3Oy"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100831</th>\n",
       "      <td>610</td>\n",
       "      <td>166534</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1493848402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100832</th>\n",
       "      <td>610</td>\n",
       "      <td>168248</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1493850091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100833</th>\n",
       "      <td>610</td>\n",
       "      <td>168250</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1494273047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100834</th>\n",
       "      <td>610</td>\n",
       "      <td>168252</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1493846352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100835</th>\n",
       "      <td>610</td>\n",
       "      <td>170875</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1493846415</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100836 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        userId  movieId  rating   timestamp\n",
       "0            1        1     4.0   964982703\n",
       "1            1        3     4.0   964981247\n",
       "2            1        6     4.0   964982224\n",
       "3            1       47     5.0   964983815\n",
       "4            1       50     5.0   964982931\n",
       "...        ...      ...     ...         ...\n",
       "100831     610   166534     4.0  1493848402\n",
       "100832     610   168248     5.0  1493850091\n",
       "100833     610   168250     5.0  1494273047\n",
       "100834     610   168252     5.0  1493846352\n",
       "100835     610   170875     3.0  1493846415\n",
       "\n",
       "[100836 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython import get_ipython\n",
    "get_ipython().magic('reset -sf') \n",
    "\n",
    "import pandas as pd\n",
    "# read user ratings\n",
    "allRatings = pd.read_csv(\"ml-latest-small/ratings.csv\")\n",
    "allRatings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QfwWA4d2Ne6r"
   },
   "source": [
    "### Step 1: Implement Apriori Algorithm\n",
    "In this section, you need to implement the Apriori algorithm, we will check the correctness of your code and we encourage efficient implementation and skills of pruning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total row of the Original allRatings :  100836\n",
      "Only consider rating >= 3                :  81763\n"
     ]
    }
   ],
   "source": [
    "liked_movies = allRatings[allRatings['rating'] >=3 ]\n",
    "print(\"The total row of the Original allRatings : \", len(allRatings))\n",
    "print(\"Only consider rating >= 3                : \", len(liked_movies))\n",
    "\n",
    "l = liked_movies.groupby('userId').movieId.apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "48ITOQG09fT2"
   },
   "outputs": [],
   "source": [
    "from nltk import flatten\n",
    "import itertools\n",
    "\n",
    "def subsets(numbers):\n",
    "    if numbers == []:\n",
    "        return [[]]\n",
    "    x = subsets(numbers[1:])\n",
    "    \n",
    "    return x + [[numbers[0]] + y for y in x]\n",
    " \n",
    "# wrapper function\n",
    "def subsets_of_given_size(numbers, n):\n",
    "    return [x for x in subsets(numbers) if len(x)==n]\n",
    "\n",
    "def apriori_implementation(result, sub_size, pruned_sets, t, prev, minsup, list_l, df_ft):    \n",
    "\n",
    "    result_len = len(result)\n",
    "    \n",
    "    # PRUNING APPLIES BEFORE COUNTING FREQUENT SETS\n",
    "    # WHEN THE SIZE OF THE SUBSET IS 1, I REMOVE THE ITEM FROM THE TRANSACTION LIST IF IT IS NOT FREQUENT\n",
    "    # SO PRUNING STARTS AT THE SUBSET SIZE 3\n",
    "    # THE PARAMETER \"PRUNED_SETS\" CONTAINS ITEMS THAT ARE NOT FREQUENT\n",
    "    if (sub_size >= 3):\n",
    "        for j in pruned_sets:\n",
    "            index = 0\n",
    "            count = 0\n",
    "            while(count < result_len):\n",
    "                set1 = set(j)\n",
    "                set2 = set(result[index])\n",
    "                if(set1.issubset(set2)):\n",
    "                    result.remove(result[index])\n",
    "                    result_len = result_len - 1\n",
    "                index = index + 1\n",
    "                count = count + 1\n",
    "    freq = [0] * len(result)\n",
    "\n",
    "    index = 0 \n",
    "    \n",
    "    # COUNTING FREQUENCIES\n",
    "    for i in result:\n",
    "        for j in list_l:\n",
    "            if isinstance(i, list):\n",
    "                set1 = set(i)\n",
    "                set2 = set(j) \n",
    "                if(set1.issubset(set2)):\n",
    "                    freq[index] = freq[index] + 1            \n",
    "            else:\n",
    "                if i in j:\n",
    "                    freq[index] = freq[index] + 1   \n",
    "            \n",
    "        index = index + 1\n",
    "    \n",
    "\n",
    "\n",
    "    index = 0\n",
    "    \n",
    "    # APPEND TO DATAFRAME \n",
    "    # ID | SUPPORT | FREQUENCIES\n",
    "    for i in freq:        \n",
    "        if (i>=minsup):\n",
    "            t_pd= pd.DataFrame({'ID': [str(result[index])], 'Support': [\"{:.4f}\".format(i/t)], 'Freq':i})\n",
    "            df_ft = df_ft.append(t_pd)\n",
    "            \n",
    "            if (sub_size != 2):\n",
    "                prev.append(result[index])\n",
    "        index = index + 1\n",
    "    \n",
    "\n",
    "    # IF THERE IS NO \n",
    "    if(len(result) == 0):\n",
    "        return result, pruned_sets , prev, df_ft\n",
    "\n",
    "    # IF ITEMS ARE NOT FREQUENT, APPEND THEM TO PRUNED_SETS\n",
    "    # WE ARE GOING TO USE THE VARIABLE WHEN WE PRUNE ITEMS\n",
    "    index = 0    \n",
    "    for i in freq:\n",
    "        if ( i < minsup):\n",
    "            if (sub_size == 2):\n",
    "                result.remove(result[index])\n",
    "                index = index - 1\n",
    "            else:           \n",
    "                pruned_sets.append(result[index])\n",
    "        index = index + 1\n",
    "        \n",
    "\n",
    "    # WHEN THE SIZE OF THE SUBSET IS 2, JOIN THOSE FREQUENT ITEMS\n",
    "    if (sub_size > 2):\n",
    "        index = 0\n",
    "        r_l = len(result)\n",
    "        new_result = []\n",
    "        \n",
    "        for i in range (0, r_l, 1):\n",
    "            for j in range (i+1, r_l, 1):\n",
    "                a = set(result[i])\n",
    "                b = set(result[j])    \n",
    "                sub_list = list(a.union(b))\n",
    "                \n",
    "                if(len(sub_list) == sub_size):\n",
    "                        new_result.append(sub_list)\n",
    "        \n",
    "        new_result.sort()\n",
    "        \n",
    "        return list(new_result for new_result,_ in itertools.groupby(new_result)),pruned_sets, prev, df_ft\n",
    "\n",
    "    else:\n",
    "        final = list(itertools.combinations(result, sub_size))\n",
    "        for i in range(len(final)):\n",
    "            final[i] = list(final[i])\n",
    "    \n",
    "    return final, pruned_sets, prev, df_ft\n",
    "\n",
    "def apriori_data_preprocess(transactions):\n",
    "\n",
    "    transactions = transactions.values.tolist()\n",
    "\n",
    "    each_items = sum(transactions, [])\n",
    "\n",
    "    each_items = list(set(each_items))\n",
    "\n",
    "    each_items.sort()\n",
    "\n",
    "    total_result = len(transactions)\n",
    "    \n",
    "    return each_items, total_result, transactions\n",
    "\n",
    "#item_sets, total_item_len, list_l = apriori_data_preprocess(l)\n",
    "\n",
    "def apriori_(transactions, minsup):\n",
    "    \n",
    "    each_items, t, list_l = apriori_data_preprocess(transactions)\n",
    "    \n",
    "    sub_size = 2\n",
    "\n",
    "    pruned_list = []\n",
    "\n",
    "    prev = []\n",
    "\n",
    "    df_ft= pd.DataFrame()\n",
    "\n",
    "    f, pruned_list, prev, df_ft = apriori_implementation(each_items, sub_size, pruned_list, t, prev, minsup, list_l, df_ft)\n",
    "\n",
    "    while (len(f) > 0):\n",
    "        sub_size = sub_size + 1        \n",
    "        f ,pruned_list, prev, df_ft = apriori_implementation(f, sub_size, pruned_list, t, prev, minsup, list_l, df_ft)    \n",
    "        \n",
    "    return df_ft, prev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "minsup = 150\n",
    "\n",
    "df_ft, prev = apriori_(l, minsup) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Support</th>\n",
       "      <th>Freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.3268</td>\n",
       "      <td>199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32</td>\n",
       "      <td>0.2808</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47</td>\n",
       "      <td>0.3136</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>0.3235</td>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>110</td>\n",
       "      <td>0.3580</td>\n",
       "      <td>218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[4993, 5952]</td>\n",
       "      <td>0.2545</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[4993, 7153]</td>\n",
       "      <td>0.2545</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[5952, 7153]</td>\n",
       "      <td>0.2463</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[296, 356, 318]</td>\n",
       "      <td>0.2644</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[296, 593, 318]</td>\n",
       "      <td>0.2479</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID Support  Freq\n",
       "0                 1  0.3268   199\n",
       "0                32  0.2808   171\n",
       "0                47  0.3136   191\n",
       "0                50  0.3235   197\n",
       "0               110  0.3580   218\n",
       "..              ...     ...   ...\n",
       "0      [4993, 5952]  0.2545   155\n",
       "0      [4993, 7153]  0.2545   155\n",
       "0      [5952, 7153]  0.2463   150\n",
       "0   [296, 356, 318]  0.2644   161\n",
       "0   [296, 593, 318]  0.2479   151\n",
       "\n",
       "[69 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ea6GbdOOBZOY"
   },
   "source": [
    "### Step 2: Print Your Association Rules\n",
    "\n",
    "Next you should print your final association rules in the following format:\n",
    "\n",
    "**movie_name_1, movie_name_2, ... --> \n",
    "movie_name_k**\n",
    "\n",
    "where the movie names can be fetched by joining the movieId with the file `movies.csv`. For example, one rule that you might find is:\n",
    "\n",
    "**Matrix, The (1999),  Star Wars: Episode V - The Empire Strikes Back (1980),  Star Wars: Episode IV - A New Hope (1977),  -> \n",
    "Star Wars: Episode VI - Return of the Jedi (1983)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "rSbhiIkw9kj1"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "movies = pd.read_csv('ml-latest-small/movies.csv')\n",
    "\n",
    "def Association_Rules(frequent_lists, minconf):\n",
    "    \n",
    "    for i in frequent_lists:\n",
    "        association_list = []\n",
    "        for j in range (1,len(i), 1):\n",
    "            association_list = association_list + (subsets_of_given_size(i,j))\n",
    "\n",
    "        for j in association_list:\n",
    "            if(len(j) == 1):\n",
    "                    bottom = df_ft[df_ft['ID'] == str(j[0])]['Freq']\n",
    "            else:\n",
    "                    j.sort()\n",
    "                    bottom = df_ft[df_ft['ID'] == str(j)]['Freq']                    \n",
    "        \n",
    "            if (len(bottom) != 0):\n",
    "                bottom = float(bottom)\n",
    "            else:\n",
    "                bottom = 0\n",
    "                \n",
    "            up = float(df_ft[df_ft['ID'] == str(i)]['Freq'])\n",
    "\n",
    "            if(bottom != 0):\n",
    "                if (up/bottom > minconf):\n",
    "                    s_r = ''\n",
    "                    s_l = ''\n",
    "                    for m_id in list(set(i) - set(j)):\n",
    "                        s_r += str(movies[movies['movieId']==m_id]['title'].to_string(index=False))\n",
    "                \n",
    "                    for m_id in j:\n",
    "                        s_l += str(movies[movies['movieId']==m_id]['title'].to_string(index=False))\n",
    "                    print(s_l, \" -> \", s_r)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Seven (a.k.a. Se7en) (1995)  ->   Pulp Fiction (1994)\n",
      " Apollo 13 (1995)  ->   Forrest Gump (1994)\n",
      " Star Wars: Episode V - The Empire Strikes Back...  ->   Star Wars: Episode IV - A New Hope (1977)\n",
      " Star Wars: Episode VI - Return of the Jedi (1983)  ->   Star Wars: Episode IV - A New Hope (1977)\n",
      " Jurassic Park (1993)  ->   Forrest Gump (1994)\n",
      " Star Wars: Episode VI - Return of the Jedi (1983)  ->   Star Wars: Episode V - The Empire Strikes Back...\n",
      " Lord of the Rings: The Two Towers, The (2002)  ->   Lord of the Rings: The Fellowship of the Ring,...\n",
      " Lord of the Rings: The Fellowship of the Ring,...  ->   Lord of the Rings: The Two Towers, The (2002)\n",
      " Lord of the Rings: The Return of the King, The...  ->   Lord of the Rings: The Fellowship of the Ring,...\n",
      " Lord of the Rings: The Fellowship of the Ring,...  ->   Lord of the Rings: The Return of the King, The...\n",
      " Lord of the Rings: The Return of the King, The...  ->   Lord of the Rings: The Two Towers, The (2002)\n",
      " Lord of the Rings: The Two Towers, The (2002)  ->   Lord of the Rings: The Return of the King, The...\n",
      " Shawshank Redemption, The (1994) Silence of the Lambs, The (1991)  ->   Pulp Fiction (1994)\n",
      " Pulp Fiction (1994) Silence of the Lambs, The (1991)  ->   Shawshank Redemption, The (1994)\n"
     ]
    }
   ],
   "source": [
    "minconf = 0.8\n",
    "\n",
    "Association_Rules(prev, minconf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FfeufQAxNB82"
   },
   "source": [
    "### Step 3: Implement Random Sampling\n",
    "\n",
    "We discussed in class a method to randomly sample baskets to avoid the overhead of reading the entire set of baskets (which in practice, could amount to billions of baskets). For this part, you should implement such a random sampling approach that takes a special parameter **alpha** that controls the size of the sample: e.g., alpha = 0.10 means to sample 10% of the baskets (our users, in this case). \n",
    "\n",
    "Vary **alpha** and report the number of frequent itemsets you find and how this compares to the number of frequent itemsets in the entire dataset. What do you discover?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "bslz87rc9kET"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "def apriori_random_sampled(alpha, minsup):\n",
    "\n",
    "    random_state=np.random.RandomState(123)\n",
    "\n",
    "    allRatings = pd.read_csv(\"ml-latest-small/ratings.csv\")\n",
    "\n",
    "    allRatings = allRatings.sample(frac= alpha, random_state = random_state )\n",
    "\n",
    "    liked_movies = allRatings[allRatings['rating'] >=3 ]\n",
    "    \n",
    "    l = liked_movies.groupby('userId').movieId.apply(list)\n",
    "    \n",
    "    new_result, p = apriori_(l, minsup) \n",
    "    \n",
    "    return new_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total frequent items:  0\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "alpha = .1\n",
    "minsup = 150\n",
    "\n",
    "frequent_items = apriori_random_sampled(alpha,minsup)\n",
    "print(\"Total frequent items: \", len(frequent_items))\n",
    "print(frequent_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total frequent items:  1\n",
      "    ID Support  Freq\n",
      "0  318  0.2841   173\n"
     ]
    }
   ],
   "source": [
    "alpha = .5\n",
    "\n",
    "frequent_items = apriori_random_sampled(alpha,minsup)\n",
    "print(\"Total frequent items: \", len(frequent_items))\n",
    "print(frequent_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total frequent items:  4\n",
      "    ID Support  Freq\n",
      "0  296  0.2939   179\n",
      "0  318  0.3350   204\n",
      "0  356  0.2989   182\n",
      "0  593  0.2611   159\n"
     ]
    }
   ],
   "source": [
    "alpha = .6\n",
    "\n",
    "frequent_items = apriori_random_sampled(alpha,minsup)\n",
    "print(\"Total frequent items: \", len(frequent_items))\n",
    "print(frequent_items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total frequent items:  19\n",
      "     ID Support  Freq\n",
      "0     1  0.2726   166\n",
      "0    47  0.2545   155\n",
      "0    50  0.2529   154\n",
      "0   110  0.2841   173\n",
      "0   260  0.3071   187\n",
      "0   296  0.3892   237\n",
      "0   318  0.4236   258\n",
      "0   356  0.4072   248\n",
      "0   480  0.2923   178\n",
      "0   527  0.2742   167\n",
      "0   589  0.2808   171\n",
      "0   593  0.3530   215\n",
      "0  1196  0.2644   161\n",
      "0  1198  0.2578   157\n",
      "0  1210  0.2479   151\n",
      "0  2571  0.3251   198\n",
      "0  2858  0.2529   154\n",
      "0  2959  0.2808   171\n",
      "0  4993  0.2463   150\n"
     ]
    }
   ],
   "source": [
    "alpha = .8\n",
    "\n",
    "frequent_items = apriori_random_sampled(alpha,minsup)\n",
    "print(\"Total frequent items: \", len(frequent_items))\n",
    "print(frequent_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rIBMkM-W-8Tn"
   },
   "source": [
    "*your discussion here*\n",
    "\n",
    "alpha: 0.1\n",
    "Total frequent items: 0\n",
    "\n",
    "alpha: 0.5\n",
    "Total frequent items: 1\n",
    "\n",
    "alpha: 0.6\n",
    "Total frequent items: 4\n",
    "\n",
    "alpha: 0.8\n",
    "Total frequent items: 19\n",
    "\n",
    "alpha: 1\n",
    "Total frequent items: 69\n",
    "\n",
    "Since the amount of items from the sample size is smaller than the entire items, there are less frequencies because we are missing many items. However, getting the frequencies from those samples were much faster than getting frequencies from the entire datatsets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wLNDiFDrI2-2"
   },
   "source": [
    "### Step 4: Check for False Positives\n",
    "\n",
    "Next you should verify that the candidate pairs you discover by random sampling are truly frequent by comparing to the itemsets you discover over the entire dataset. \n",
    "\n",
    "For this part, consider another parameter **minsup_sample** that relaxes the minimum support threshold. For example if we want minsup = 1/100 for whole dataset, then try minsup_sample = 1/125 for the sample. This will help catch truly frequent itemsets.\n",
    "\n",
    "Vary **minsup_sample** and report the number of frequent itemsets you find and the number of false positives you find. What do you discover?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "R8gtIPuf_82B"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minsup              :  12.0\n",
      "Total frequent items:  81\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "alpha = .1\n",
    "minsup_sample = alpha * (minsup*.8)\n",
    "frequent_items = apriori_random_sampled(alpha, int(minsup_sample))\n",
    "print(\"Minsup              : \", minsup_sample)\n",
    "print(\"Total frequent items: \", len(frequent_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minsup              :  13\n",
      "Minsup              :  13\n",
      "Total frequent items:  65\n"
     ]
    }
   ],
   "source": [
    "alpha = .1\n",
    "minsup_sample = alpha * (minsup * .9)\n",
    "print(\"Minsup              : \", int(minsup_sample))\n",
    "frequent_items = apriori_random_sampled(alpha,int(minsup_sample))\n",
    "print(\"Minsup              : \", int(minsup_sample))\n",
    "print(\"Total frequent items: \", len(frequent_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minsup              :  56\n",
      "Total frequent items:  78\n"
     ]
    }
   ],
   "source": [
    "alpha = .5\n",
    "minsup_sample = alpha * (minsup * .75)\n",
    "frequent_items = apriori_random_sampled(alpha,int(minsup_sample))\n",
    "print(\"Minsup              : \", int(minsup_sample))\n",
    "print(\"Total frequent items: \", len(frequent_items))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tgpWjmXh_91h"
   },
   "source": [
    "*your discussion here*\n",
    "\n",
    "Varing minsup_smaple gives more candidate frequent items from samples compared to Step 3. Items that are frequent in those samples may not be frequent in overall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wt4JCrwcAHal"
   },
   "source": [
    "### Step 5: Extensions and Next Steps\n",
    "\n",
    "So far, we have been working with a fairly small dataset. For this last question, try your sampling-based approach on the much larger: **Movies 10M** dataset: https://files.grouplens.org/datasets/movielens/ml-10m.zip\n",
    "\n",
    "First, we need to load this larger dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "iG7qBkj7AVou"
   },
   "outputs": [],
   "source": [
    "import urllib3\n",
    "import zipfile\n",
    "\n",
    "http = urllib3.PoolManager()\n",
    "req = http.request(\"GET\", \"https://files.grouplens.org/datasets/movielens/ml-10m.zip\", preload_content=False)\n",
    "\n",
    "with open(\"movie.zip\", 'wb') as out:\n",
    "  while True:\n",
    "    data = req.read(4096)\n",
    "    if not data:\n",
    "      break\n",
    "    out.write(data)\n",
    "req.release_conn()\n",
    "\n",
    "zFile = zipfile.ZipFile(\"movie.zip\", \"r\")\n",
    "for fileM in zFile.namelist():\n",
    "  zFile.extract(fileM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "6Hi45CqJht7n"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.html      movies.dat       split_ratings.sh\r\n",
      "allbut.pl        ratings.dat      tags.dat\r\n"
     ]
    }
   ],
   "source": [
    "! ls ml-10M100K/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "V_jdR72WiR2F"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>122</td>\n",
       "      <td>5.0</td>\n",
       "      <td>838985046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>185</td>\n",
       "      <td>5.0</td>\n",
       "      <td>838983525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>231</td>\n",
       "      <td>5.0</td>\n",
       "      <td>838983392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>292</td>\n",
       "      <td>5.0</td>\n",
       "      <td>838983421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>316</td>\n",
       "      <td>5.0</td>\n",
       "      <td>838983392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000049</th>\n",
       "      <td>71567</td>\n",
       "      <td>2107</td>\n",
       "      <td>1.0</td>\n",
       "      <td>912580553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000050</th>\n",
       "      <td>71567</td>\n",
       "      <td>2126</td>\n",
       "      <td>2.0</td>\n",
       "      <td>912649143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000051</th>\n",
       "      <td>71567</td>\n",
       "      <td>2294</td>\n",
       "      <td>5.0</td>\n",
       "      <td>912577968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000052</th>\n",
       "      <td>71567</td>\n",
       "      <td>2338</td>\n",
       "      <td>2.0</td>\n",
       "      <td>912578016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000053</th>\n",
       "      <td>71567</td>\n",
       "      <td>2384</td>\n",
       "      <td>2.0</td>\n",
       "      <td>912578173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000054 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          userId  movieId  rating  timestamp\n",
       "0              1      122     5.0  838985046\n",
       "1              1      185     5.0  838983525\n",
       "2              1      231     5.0  838983392\n",
       "3              1      292     5.0  838983421\n",
       "4              1      316     5.0  838983392\n",
       "...          ...      ...     ...        ...\n",
       "10000049   71567     2107     1.0  912580553\n",
       "10000050   71567     2126     2.0  912649143\n",
       "10000051   71567     2294     5.0  912577968\n",
       "10000052   71567     2338     2.0  912578016\n",
       "10000053   71567     2384     2.0  912578173\n",
       "\n",
       "[10000054 rows x 4 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# read user ratings\n",
    "allRatings = pd.read_csv(\"ml-10M100K/ratings.dat\",sep='::', names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"], engine='python')\n",
    "allRatings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WEX9wu7ewIqb"
   },
   "source": [
    "Now you can begin your sampling over this larger dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "VYRlIEyulq2X"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Frequent items from sampling:  39\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "def step_5(allRatings, alpha, minsup):\n",
    "\n",
    "    random_state=np.random.RandomState(123)\n",
    "\n",
    "    allRatings = allRatings.sample(frac= alpha, random_state = random_state )\n",
    "\n",
    "    liked_movies = allRatings[allRatings['rating'] >=3 ]\n",
    "    \n",
    "    l = liked_movies.groupby('userId').movieId.apply(list)\n",
    "    \n",
    "    new_result, p = apriori_(l, minsup) \n",
    "    \n",
    "    return new_result\n",
    "\n",
    "alpha= 0.05\n",
    "minsup = 800\n",
    "\n",
    "print(\"Number of Frequent items from sampling: \", len(step_5(allRatings, alpha, minsup)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FX3LDkfAlpyg"
   },
   "source": [
    "*your discussion here*\n",
    "\n",
    "Implementing the algorithm with 10M dataset takes much time than the dataset that we used above, so sampling in this case is very useful because it is much faster. When we have a time constraint there could be possibilities that we cannot produce those frequent itemsets. However, Sampling makes us possible to produce frequencies with short amount of time. However, there are still possibilities that we can miss bunch of items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "homework_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
